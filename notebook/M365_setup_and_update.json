{
	"name": "M365_setup_and_update",
	"properties": {
		"nbformat": 0,
		"nbformat_minor": 0,
		"bigDataPool": {
			"referenceName": "spark1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/88bc4ad2-b92a-4cec-9c63-bac0d9696a30/resourceGroups/rg-oea-esola/providers/Microsoft.Synapse/workspaces/syn-oea-esola/bigDataPools/spark1",
				"name": "spark1",
				"type": "Spark",
				"endpoint": "https://syn-oea-esola.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## M365 module setup\n",
					"\n",
					"\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storage_account = 'stoeaesola'\n",
					"use_test_env = False\n",
					"has_calendar_records = False\n",
					"has_course_records = False\n",
					"has_org_records = False\n",
					"has_person_records = False\n",
					"has_person_identifier_records = False\n",
					"has_ref_definition_records = False\n",
					"has_section_records = False\n",
					"has_session_records = False\n",
					"has_staff_org_affiliation_records = False\n",
					"has_student_org_affiliation_records = False\n",
					"has_staff_section_membership_records = False\n",
					"has_student_section_membership_records = False"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"if use_test_env:\n",
					"    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\n",
					"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\n",
					"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\n",
					"else:\n",
					"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
					"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
					"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"stage1_m365 = stage1 + '/m365/DIPData'\n",
					"stage1_m365_activity = stage1 + '/m365/DIPData/Activity/ApplicationUsage'\n",
					"\n",
					"# Process Roster data from stage 1 to stage 2 (adds schema info and writes out in parquet format)\n",
					"\n",
					"# Calendar\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/Calendar.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_calendar_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'Calendar')\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Description, cast(_c3 as int) SchoolYear, cast(_c4 as boolean) IsCurrent, _c5 ExternalId, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c7, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c8 as boolean) IsActive, _c9 OrgId from Calendar\")\n",
					"  df.write.format(\"parquet\").mode(\"overwrite\").save(stage2 + '/m365/Calendar')\n",
					"# Course\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/Course.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_course_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'Course')\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Description, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId from Course\")\n",
					"  df.write.format(\"parquet\").mode(\"overwrite\").save(stage2 + '/m365/Course')\n",
					"# Person\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/Person.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_person_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'Person')\n",
					"  df_Person = spark.sql(\"select _c0 Id, _c1 FirstName, _c2 MiddleName, _c3 LastName, _c4 GenerationCode, _c5 Prefix, _c6 EnabledUser, _c7 ExternalId, to_timestamp(_c8, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c9, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c10 as boolean) IsActive, _c11 SourceSystemId from Person\")\n",
					"  df_Person.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Person')\n",
					"# PersonIdentifier\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/PersonIdentifier.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_person_identifier_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'PersonIdentifier')\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Identifier, _c2 Description, _c3 RefIdentifierTypeId, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 SourceSystemId from PersonIdentifier\")\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/PersonIdentifier')\n",
					"# RefDefinition\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/RefDefinition.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_ref_definition_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'RefDefinition')\n",
					"  df = spark.sql(\"select _c0 Id, _c1 RefType, _c2 Namespace, _c3 Code, cast(_c4 as int) SortOrder, _c5 Description, cast(_c6 as boolean) IsActive from RefDefinition\")\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/RefDefinition')\n",
					"# Section\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/Section.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_section_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'Section')\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Location, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CourseId, _c9 RefSectionTypeId, _c10 SessionId, _c11 OrgId from Section\")\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Section')\n",
					"# Session\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/Session.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_session_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'Session')\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Name, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') BeginDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') EndDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId, _c9 ParentSessionId, _c10 RefSessionTypeId from Session\")\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Session')\n",
					"# StaffOrgAffiliation\n",
					"dfstaffoa = spark.read.csv(stage1_m365 + '/Roster/StaffOrgAffiliation.csv', header='false')\n",
					"if (dfstaffoa.count() > 0):\n",
					"  has_staff_org_affiliation_records = True\n",
					"  sqlContext.registerDataFrameAsTable(dfstaffoa, 'StaffOrgAffiliation')\n",
					"  dfstaffoa = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefStaffOrgRoleId from StaffOrgAffiliation\")\n",
					"  dfstaffoa.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StaffOrgAffiliation')\n",
					"# StaffSectionMembership\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/StaffSectionMembership.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_staff_section_membership_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'StaffSectionMembership')\n",
					"  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimaryStaffForSection, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 RefStaffSectionRoleId, _c10 SectionId from StaffSectionMembership\")\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StaffSectionMembership')\n",
					"# StudentOrgAffiliation\n",
					"dfsoa = spark.read.csv(stage1_m365 + '/Roster/StudentOrgAffiliation.csv', header='false')\n",
					"if (dfsoa.count() > 0):\n",
					"  has_student_org_affiliation_records = True\n",
					"  sqlContext.registerDataFrameAsTable(dfsoa, 'StudentOrgAffiliation')\n",
					"  dfsoa = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefGradeLevelId, _c11 RefStudentOrgRoleId, _c12 RefEnrollmentStatusId from StudentOrgAffiliation\")\n",
					"  dfsoa.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StudentOrgAffiliation')\n",
					"# StudentSectionMembership\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/StudentSectionMembership.csv', header='false')\n",
					"if (df.count() > 0):\n",
					"  has_student_section_membership_records = True\n",
					"  sqlContext.registerDataFrameAsTable(df, 'StudentSectionMembership')\n",
					"  df = spark.sql(\"select _c0 Id, to_timestamp(_c1, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 PersonId, _c8 RefGradeLevelWhenCourseTakenId, _c9 RefStudentSectionRoleId, _c10 SectionId from StudentSectionMembership\")\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StudentSectionMembership')"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Org\r\n",
					"from pyspark.sql.functions import countDistinct\r\n",
					"df = spark.read.csv(stage1_m365 + '/Roster/Org.csv', header='false')\r\n",
					"if (df.count() > 0):\r\n",
					"  has_org_records = True\r\n",
					"  sqlContext.registerDataFrameAsTable(df, 'Org')\r\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Identifier, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 ParentOrgId, _c8 RefOrgTypeId, _c9 SourceSystemId from Org\")\r\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Org')\r\n",
					"\r\n",
					"  dfsoagrouped = dfsoa.groupBy(\"OrgId\").agg(countDistinct(\"personId\").alias(\"SumStudents\"))\r\n",
					"  dfstaffoagrouped = dfstaffoa.groupBy(\"OrgId\").agg(countDistinct(\"personId\").alias(\"SumStaff\"))\r\n",
					"  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Identifier, _c3 ExternalId from Org\")\r\n",
					"  df = df.join(dfsoagrouped, df[\"Id\"] == dfsoagrouped[\"OrgId\"] ,\"left\").drop('OrgId').select(df['*'], dfsoagrouped[\"SumStudents\"]).na.fill(0)\r\n",
					"  df = df.join(dfstaffoagrouped, df[\"Id\"] == dfstaffoagrouped[\"OrgId\"] ,\"left\").drop('OrgId').select(df['*'], dfstaffoagrouped[\"SumStaff\"]).na.fill(0)\r\n",
					"  \r\n",
					"  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/OrgSummary')"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"# Process Activity data from stage1 into stage2.\n",
					"#\n",
					"# If this is the first load, it loads all activity data.\n",
					"# If this is a subsequent load, it determines the max date currently stored and only loads data from after that date.\n",
					"\n",
					"def append_to_activity_table(max_date=False):\n",
					"    df = spark.read.csv(stage1_m365_activity, header='false') \n",
					"    sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
					"    df_Activity = spark.sql(\"select _c0 SignalType, to_timestamp(_c1) StartTime, _c2 UserAgent, _c3 SignalId, _c4 SISClassId, _c5 OfficeClassId, _c6 ChannelId, _c7 AppName, _c8 ActorId, _c9 ActorRole, _c10 SchemaVersion, _c11 AssignmentId, _c12 SubmissionId, _c13 Action, _c14 AssginmentDueDate, _c15 ClassCreationDate, _c16 Grade, _c17 SourceFileExtension, _c18 MeetingDuration, '' PersonId from Activity\")    \n",
					"  \n",
					"    if (max_date):\n",
					"      df_Activity = df_Activity.filter(df_Activity.StartTime > max_date)\n",
					"\n",
					"    if (df_Activity.count() == 0):\n",
					"        print('No new activity data to load')\n",
					"    else:\n",
					"        print('Adding activity data later than: ' + str(max_date))\n",
					"        # The assumption here is that there will always be data in these inbound files\n",
					"        sqlContext.registerDataFrameAsTable(df_Activity, 'Activity')\n",
					"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/PersonIdentifier'), 'PersonIdentifier')\n",
					"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/RefDefinition'), 'RefDefinition')\n",
					"   \n",
					"        df1 = spark.sql( \\\n",
					"        \"select act.SignalType, act.StartTime, act.UserAgent, act.SignalId, act.SISClassId, act.OfficeClassId, act.ChannelId, \\\n",
					"        act.AppName, act.ActorId, act.ActorRole, act.SchemaVersion, act.AssignmentId, act.SubmissionId, act.Action, act.AssginmentDueDate, \\\n",
					"        act.ClassCreationDate, act.Grade, act.SourceFileExtension, act.MeetingDuration, pi.PersonId \\\n",
					"        from PersonIdentifier pi, RefDefinition rd, Activity act \\\n",
					"        where \\\n",
					"            pi.RefIdentifierTypeId = rd.Id \\\n",
					"            and rd.RefType = 'RefIdentifierType' \\\n",
					"            and rd.Code = 'ActiveDirectoryId' \\\n",
					"            and pi.Identifier = act.ActorId \\\n",
					"        \")\n",
					"\n",
					"        df1.write.format(\"parquet\").mode(\"append\").save(stage2 + '/m365/Activity0p2')\n",
					"\n",
					"try:\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/Activity0p2')\n",
					"    sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
					"    # Bad data with a date in the future can prevent the uploading of new activity data, \n",
					"    # so we ensure that the watermark is calculated on good data by filtering with CURRENT_TIMESTAMP\n",
					"    df1 = spark.sql(\"select StartTime from Activity where StartTime < CURRENT_TIMESTAMP\")\n",
					"    max_date = df1.agg({'StartTime': 'max'}).first()[0]\n",
					"    print(max_date)\n",
					"    append_to_activity_table(max_date)    \n",
					"except:\n",
					"    print(\"No Activity data has been loaded into stage2 data lake yet.\")\n",
					"    append_to_activity_table()\n",
					"\n",
					""
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Org\r\n",
					"if (has_org_records):\r\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/Org').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Org')\r\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/OrgSummary').write.format('parquet').mode('overwrite').save(stage3 + '/m365/OrgSummary')\r\n",
					""
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"# Anonymize the M365 data from stage2 and load into stage3\n",
					"\n",
					"from pyspark.sql.functions import sha2, lit\n",
					"# Activity\n",
					"df = spark.read.format('parquet').load(stage2 + '/m365/Activity0p2')\n",
					"df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ActorId', sha2(df.ActorId, 256))\n",
					"df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/Activity0p2')\n",
					"# Calendar, Course, Org\n",
					"if (has_calendar_records):\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/Calendar').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Calendar')\n",
					"\n",
					"if (has_course_records):\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/Course').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Course')\n",
					"\n",
					"# Person\n",
					"if (has_person_records):\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/Person')\n",
					"    df = df.withColumn('Id', sha2(df.Id, 256)).withColumn('FirstName', lit('*')).withColumn(\"MiddleName\", lit('*')).withColumn('LastName', lit('*')).withColumn('ExternalId', sha2(df.ExternalId, 256))\n",
					"    df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/Person')\n",
					"\n",
					"# PersonIdentifier\n",
					"if (has_person_identifier_records):\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/PersonIdentifier')\n",
					"    df = df.withColumn('PersonId', sha2(df.Id, 256)).withColumn('Identifier', lit('*')).withColumn(\"ExternalId\", lit('*'))\n",
					"    df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/PersonIdentifier')\n",
					"\n",
					"# RefDefinition, Section, Session\n",
					"if (has_ref_definition_records):\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/RefDefinition').write.format('parquet').mode('overwrite').save(stage3 + '/m365/RefDefinition')\n",
					"\n",
					"if (has_section_records):\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/Section').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Section')\n",
					"\n",
					"if (has_session_records):\n",
					"    spark.read.format('parquet').load(stage2 + '/m365/Session').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Session')\n",
					"\n",
					"# StaffOrgAffiliation\n",
					"if (has_staff_org_affiliation_records):\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/StaffOrgAffiliation')\n",
					"    df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
					"    df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StaffOrgAffiliation')\n",
					"\n",
					"# StaffSectionMembership\n",
					"if (has_staff_section_membership_records):\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/StaffSectionMembership')\n",
					"    df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
					"    df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StaffSectionMembership')\n",
					"\n",
					"# StudentOrgAffiliation\n",
					"if (has_student_org_affiliation_records):\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/StudentOrgAffiliation')\n",
					"    df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
					"    df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StudentOrgAffiliation')\n",
					"\n",
					"# StudentSectionMembership\n",
					"if (has_student_section_membership_records):\n",
					"    df = spark.read.format('parquet').load(stage2 + '/m365/StudentSectionMembership')\n",
					"    df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
					"    df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StudentSectionMembership')"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
					"# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
					"# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
					"def create_spark_db(db_name, source_path):\n",
					"    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
					"    spark.sql(\"create table if not exists \" + db_name + \".Activity using PARQUET location '\" + source_path + \"/Activity0p2'\")\n",
					"    if (has_calendar_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".Calendar using PARQUET location '\" + source_path + \"/Calendar'\")\n",
					"    \n",
					"    if (has_course_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".Course using PARQUET location '\" + source_path + \"/Course'\")\n",
					"    \n",
					"    if (has_org_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".Org using PARQUET location '\" + source_path + \"/Org'\")\n",
					"        spark.sql(\"drop table if exists \" + db_name + \".OrgSummary\")\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".OrgSummary using PARQUET location '\" + source_path + \"/OrgSummary'\")\n",
					"    \n",
					"    if (has_person_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".Person using PARQUET location '\" + source_path + \"/Person'\")\n",
					"    \n",
					"    if (has_person_identifier_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".PersonIdentifier using PARQUET location '\" + source_path + \"/PersonIdentifier'\")\n",
					"    \n",
					"    if (has_ref_definition_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".RefDefinition using PARQUET location '\" + source_path + \"/RefDefinition'\")\n",
					"    \n",
					"    if (has_section_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".Section using PARQUET location '\" + source_path + \"/Section'\")\n",
					"    \n",
					"    if (has_session_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".Session using PARQUET location '\" + source_path + \"/Session'\")\n",
					"    \n",
					"    if (has_staff_org_affiliation_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".StaffOrgAffiliation using PARQUET location '\" + source_path + \"/StaffOrgAffiliation'\")\n",
					"    \n",
					"    if (has_staff_section_membership_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".StaffSectionMembership using PARQUET location '\" + source_path + \"/StaffSectionMembership'\")\n",
					"    \n",
					"    if (has_student_org_affiliation_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".StudentOrgAffiliation using PARQUET location '\" + source_path + \"/StudentOrgAffiliation'\")\n",
					"\n",
					"    if (has_student_section_membership_records):\n",
					"        spark.sql(\"create table if not exists \" + db_name + \".StudentSectionMembership using PARQUET location '\" + source_path + \"/StudentSectionMembership'\")\n",
					"\n",
					"db_prefix = 'test_' if use_test_env else ''\n",
					"create_spark_db(db_prefix + 's2_m365', stage2 + '/m365')\n",
					"create_spark_db(db_prefix + 's3_m365', stage3 + '/m365')"
				],
				"execution_count": 32
			}
		]
	}
}